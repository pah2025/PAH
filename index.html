<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prototype-Augmented Hypernetworks for Continual Multitask Learning (PAH)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0 auto;
            max-width: 800px;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            overflow-x: auto;
        }
        code {
            font-family: monospace;
        }
        img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>

<h1>Prototype-Augmented Hypernetworks for Continual Multitask Learning (PAH)</h1>

<img src="https://github.com/user-attachments/assets/fafbe56a-a5c4-472d-8fcb-c45fd522ee78" alt="PAH scheme">

<h2>Abstract</h2>
<p>
Continual learning aims to learn sequential tasks without forgetting prior knowledge, but catastrophic forgetting—primarily concentrated in the final classification layers—remains a challenge. We propose <strong>Prototype-Augmented Hypernetworks (PAH)</strong>, a framework that uses hypernetworks conditioned on learnable task prototypes to dynamically generate task-specific classifier heads. By aligning these heads with evolving representations and preserving shared knowledge through distillation, PAH effectively mitigates catastrophic forgetting. Extensive evaluations on Split-CIFAR100 and TinyImageNet demonstrate state-of-the-art performance, achieving robust accuracy and minimal forgetting.
</p>

<h2>Key Features</h2>
<ul>
    <li><strong>Hypernetwork-based Classifier Adaptation:</strong> Dynamically generates classifier heads conditioned on task-specific prototypes.</li>
    <li><strong>Prototype Learning:</strong> Learnable task embeddings align with evolving feature representations.</li>
    <li><strong>Knowledge Distillation:</strong> Preserves shared knowledge across tasks, reducing catastrophic forgetting.</li>
    <li><strong>State-of-the-Art Performance:</strong> Demonstrated on <strong>Split-CIFAR100</strong>, <strong>TinyImageNet</strong>, and <strong>Split-MNIST</strong>, outperforming existing continual learning baselines.</li>
</ul>

<h2>Installation</h2>
<h3>Clone the Repository</h3>
<pre><code>git clone https://github.com/pah2025/PAH.git
cd PAH</code></pre>

<h3>Create Virtual Environment</h3>
<pre><code># Using venv
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate

# OR using conda
conda create -n your-env-name python=3.8
conda activate your-env-name</code></pre>

<h3>Install Dependencies</h3>
<pre><code>pip install -r requirements.txt</code></pre>

<h2>Data</h2>
<p>
Running the training script automatically downloads and saves Split-MNIST and Split-CIFAR100 data in a new folder called <code>data</code>. The TinyImageNet dataset can be downloaded from: <a href="https://paperswithcode.com/dataset/tiny-imagenet">TinyImageNet</a>.
</p>

<h2>Usage</h2>
<p>Modify <code>config/hyper2d.py</code> with desired parameters and settings, then run:</p>
<pre><code>python train_hyper2d.py config/hyper2d.py</code></pre>

<h2>Results</h2>
<p>
Results are stored in a folder called <code>results</code> and can also be viewed in Weights & Biases if configured.
</p>

<h2>License</h2>
<p>This project is licensed under the MIT License.</p>

</body>
</html>

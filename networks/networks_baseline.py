import torch

from torch import nn

class TaskHead_Baseline(nn.Module):
    def __init__(self, input_size: int, # number of features in the backbone's output
                 projection_size: int,  # number of neurons in the hidden layer
                 num_classes: int,      # number of output neurons
                 device: torch.device, # device to run on
                 dropout: float=0.0):     # optional dropout rate to apply
        super().__init__()
        
        self.projection = nn.Linear(input_size, projection_size)
        self.classifier = nn.Linear(projection_size, num_classes)

        if dropout > 0:
            self.dropout = nn.Dropout(dropout)
        else:
            self.dropout = nn.Identity()

        self.relu = nn.ReLU()

        self.device = device
        self.to(device)

    def forward(self, x):
        # assume x is already unactivated feature logits,
        # e.g. from resnet backbone
        x = self.projection(self.relu(self.dropout(x)))
        x = self.classifier(self.relu(self.dropout(x)))

        return x
    


class TaskHead_simple(nn.Module):
    def __init__(self, input_size: int, # number of features in the backbone's output
                 num_classes: int,      # number of output neurons
                 device: str,           # device for computation ('cuda' or 'cpu')
                 dropout: float=0.,     # optional dropout rate to apply
                 ):
        super().__init__()

        self.input_size = input_size
        self.num_classes = num_classes
        self.dropout = dropout
        self.device = device

        self.classifier = nn.Linear(input_size, num_classes, bias=False)

        self.device = device
        self.to(device)

    def forward(self, x):
        return self.classifier(x)
    

### and a baseline_lwf_model that contains a backbone plus multiple class heads,
### and performs task-ID routing at runtime, allowing it to perform any learned task:
class MultitaskModel_Baseline(nn.Module):
    def __init__(self, backbone: nn.Module, 
                 device):
        super().__init__()

        self.backbone = backbone.to(device)

        # for param in self.backbone.parameters():
        #     param.requires_grad = False

        # a dict mapping task IDs to the classification heads for those tasks:
        self.task_heads = nn.ModuleDict()        
        # we must use a nn.ModuleDict instead of a base python dict,
        # to ensure that the modules inside are properly registered in self.parameters() etc.

        self.relu = nn.ReLU()
        self.device = device
        self.to(device)
    

    def forward(self, x: torch.Tensor, task_id: int):
        if x.device != self.device:
            x = x.to(self.device)
        
        task_id = str(int(task_id))
        # nn.ModuleDict requires string keys for some reason,
        # so we have to be sure to cast the task_id from tensor(2) to 2 to '2'
        
        assert task_id in self.task_heads, f"no head exists for task id {task_id}"
        
        # select which classifier head to use:
        chosen_head = self.task_heads[task_id]

        # activated features from backbone:
        x = self.relu(self.backbone(x))
        # task-specific prediction:
        x = chosen_head(x)

        return x

    def add_task(self, 
                 task_id: int, 
                 head: nn.Module):
        """accepts an integer task_id and a classification head
        associated to that task.
        adds the head to this baseline_lwf_model's collection of task heads."""
        self.task_heads[str(task_id)] = head.to(self.device)
    
    
    @property
    def num_task_heads(self):
        return len(self.task_heads)



class MultitaskModel_Baseline_notaskid(nn.Module):
    def __init__(self, backbone: nn.Module, 
                 device):
        super().__init__()

        self.backbone = backbone.to(device)

        # for param in self.backbone.parameters():
        #     param.requires_grad = False

        # a dict mapping task IDs to the classification heads for those tasks:
        self.task_heads = nn.ModuleDict()        
        # we must use a nn.ModuleDict instead of a base python dict,
        # to ensure that the modules inside are properly registered in self.parameters() etc.

        self.relu = nn.ReLU()
        self.device = device
        self.to(device)
    

    def forward(self, x: torch.Tensor, task_id: int):
        if x.device != self.device:
            x = x.to(self.device)
        
        # select which classifier head to use:
        chosen_head = self.task_heads["0"]

        # activated features from backbone:
        x = self.relu(self.backbone(x))
        # task-specific prediction:
        x = chosen_head(x)

        return x

    def add_task(self, 
                 task_id: int, 
                 head: nn.Module):
        """accepts an integer task_id and a classification head
        associated to that task.
        adds the head to this baseline_lwf_model's collection of task heads."""
        self.task_heads[str(task_id)] = head.to(self.device)
    
    
    @property
    def num_task_heads(self):
        return len(self.task_heads)
